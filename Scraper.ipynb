{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import smtplib\n",
    "import time\n",
    "from datetime import datetime \n",
    "import uuid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlightScraper:\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    Instance of this class allows for scraping the basic fight data from kiwi.com website.\n",
    "    \n",
    "    ***\n",
    "\n",
    "    Attributes:\n",
    "\n",
    "    date_from : date (%Y-%m-%d)\n",
    "        starting date for the scraping process\n",
    "    date_to : date (%Y-%m-%d)\n",
    "        ending date for the scraping process\n",
    "    origin : str\n",
    "        place of origin for the flights searched, format - city-country\n",
    "    destination : str\n",
    "        desired destination for the flights searched, format - city-country\n",
    "    trip_duration_days : int\n",
    "        desired duration of the trip (in days), ex. for Fri-Sun trip, this attribute should be equal to 3\n",
    "    path_to_driver : str\n",
    "        valid path to the chrome webdriver\n",
    "    weekends_only : bool\n",
    "        optional, allows searching only for flights on the weekends (3 days, Fri - Sun)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, date_from, date_to, origin, destination, trip_duration_days, path_to_driver, weekends_only = True):\n",
    "\n",
    "        \"\"\"\n",
    "        Class initialization method. \n",
    "\n",
    "        Attributes:\n",
    "\n",
    "        date_from : date (%Y-%m-%d)\n",
    "            starting date for the scraping process\n",
    "        date_to : date (%Y-%m-%d)\n",
    "            ending date for the scraping process\n",
    "        origin : str\n",
    "            place of origin for the flights searched, format - city-country\n",
    "        destination : str\n",
    "            desired destination for the flights searched, format - city-country\n",
    "        trip_duration_days : int\n",
    "            desired duration of the trip (in days), ex. for Fri-Sun trip, this attribute should be equal to 3\n",
    "        path_to_driver : str\n",
    "            valid path to the chrome webdriver\n",
    "        weekends_only : bool\n",
    "            optional, allows searching only for flights on the weekends (3 days, Fri - Sun)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        _driver : Object\n",
    "            Instance of the Chrome Webdriver from Selenium package corresponding to the particular class instance\n",
    "\n",
    "\n",
    "        Protected Attributes responsible for collecting scraped data:\n",
    "\n",
    "        self._flight_ids = []\n",
    "        self._start_location = []\n",
    "        self._end_location = []\n",
    "        self._flight_start_dates = []\n",
    "        self._flight_end_dates = []\n",
    "        self._flight_start_times = []\n",
    "        self._flight_end_times = []\n",
    "        self._flight_durations = []\n",
    "        self._flight_prices = []\n",
    "        self._flight_origin_airports = []\n",
    "        self._flight_destination_airports = []\n",
    "        self._flight_grounds = []\n",
    "        self._is_return = []\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        self.date_from = date_from\n",
    "        self.date_to = date_to\n",
    "        self.origin = origin\n",
    "        self.destination = destination\n",
    "        self.trip_duration_days = trip_duration_days # TODO: Add functionality to make trip duration adjustable\n",
    "        self.path_to_driver = path_to_driver\n",
    "        self.weekends_only = weekends_only # TODO: Add functionality for weekend only trips \n",
    "\n",
    "        self._driver = webdriver.Chrome(executable_path=path_to_driver)\n",
    "        self._flight_ids = []\n",
    "        self._start_location = []\n",
    "        self._end_location = []\n",
    "        self._flight_start_dates = []\n",
    "        self._flight_end_dates = []\n",
    "        self._flight_start_times = []\n",
    "        self._flight_end_times = []\n",
    "        self._flight_durations = []\n",
    "        self._flight_prices = []\n",
    "        self._flight_origin_airports = []\n",
    "        self._flight_destination_airports = []\n",
    "        self._flight_grounds = []\n",
    "        self._is_return = []\n",
    "\n",
    "\n",
    "    def get_page(self, start_date, end_date):\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        Method used to load the required page (kiwi.com) with specific parameters before scraping.\n",
    "\n",
    "        Parameters:\n",
    "\n",
    "            start_date : date (%Y-%m-%d)\n",
    "                starting date for the page to search flight for\n",
    "            end_date : date (%Y-%m-%d)\n",
    "                ending date for the page to search flight for\n",
    "\n",
    "        Returns: \n",
    "            None\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # Declare url to the scraped results page following the convention 'https://www.kiwi.com/pl/search/results/origin/destination/start_date/end_date/?cabinClass=ECONOMY-false'\n",
    "        page_url = 'https://www.kiwi.com/pl/search/results/' + self.origin + '/' + self.destination + '/' + start_date + '/' + end_date + '?cabinClass=ECONOMY-false'\n",
    "\n",
    "        self._driver.get(page_url)\n",
    "\n",
    "        # Each new page can open a pop-up window about cookies. Wait for 10 seconds for it to show up then try to close it.\n",
    "        time.sleep(10)\n",
    "        try:\n",
    "            self._driver.find_element_by_xpath('//*[@id=\"cookies_accept\"]/div').click()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    def find_grounds(self):\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        Helper method used to scrape information about the number of grounds during the flight.\n",
    "\n",
    "        Parameters: \n",
    "            None\n",
    "\n",
    "        Returns:\n",
    "            grounds_count : int (non-zero)\n",
    "            Number of grounds for each particular flight. Separate for outbound and inbound flights. \n",
    "\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        # In case of no grounds the counter may not be properly displayed in the page structure\n",
    "        # To combat this issue, first a parent div containing flight infomration is selected, then a grounds count is located within this parent div\n",
    "        parent_div_xpath = '//div[contains(@data-test, \"ResultCardSectorWrapper\")]'\n",
    "        child_div_xpath = \".//div[contains(@data-test, 'StopCountBadge')]\"\n",
    "\n",
    "        parent_divs = self._driver.find_elements_by_xpath(parent_div_xpath)\n",
    "\n",
    "        child_divs = [parent_div.find_element_by_xpath(child_div_xpath) for parent_div in parent_divs] # For each flight result card find a grounds counter\n",
    "\n",
    "        ground_details = [div.text for div in child_divs]\n",
    "\n",
    "\n",
    "        def find_grounds_count(landing_string):\n",
    "\n",
    "            \"\"\"Helper function to transform the raw grounds data into numerical values\"\"\"\n",
    "\n",
    "            return int('0'+''.join(d for d in landing_string if d.isdigit())) # In case of no grounds, no number is displayed on the page, so the '0' must be added manually\n",
    "        \n",
    "        grounds_count = [find_grounds_count(s) for s in ground_details] # Use helper function to turn raw strings from the page into corresponing numbers\n",
    "\n",
    "        return grounds_count\n",
    "    \n",
    "\n",
    "    def get_weekends(self):\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        Helper method used to get all the weekends between date_from and date_to.\n",
    "\n",
    "        Parameters: \n",
    "            None\n",
    "\n",
    "        Returns:\n",
    "            weekends_zipped : List[(date (%Y-%m-%d), date (%Y-%m-%d))]\n",
    "            Pairs of dates representing the begining of a weekend (Friday) and the end of a weekend (Sunday) within the provided time window. \n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # Create list of dates for all Fridays and Sundays between two given dates\n",
    "        weekends_list = list(pd.bdate_range(start=self.date_from, end=self.date_to, freq='C', weekmask=\"Fri Sun\"))\n",
    "\n",
    "        # If the time window begins after a Friday or ends before a Sunday remove those dates from the list (we are only looking for full weekends)\n",
    "        if datetime.strptime(self.date_from, '%Y-%m-%d').weekday() in (5,6):\n",
    "\n",
    "            weekends_list = weekends_list[1:] # Remove unmached Friday\n",
    "\n",
    "        if datetime.strptime(self.date_to, '%Y-%m-%d').weekday() in (4,5):\n",
    "\n",
    "            weekends_list = weekends_list[:-1] # Remove unmached Sunday\n",
    "\n",
    "        # Divide dates into sepearate lists for Fridays (start) and Sundays (end)\n",
    "        weekend_start = weekends_list[::2]\n",
    "        weekend_end = weekends_list[1::2]\n",
    "\n",
    "        weekends_zipped = [*zip(weekend_start,weekend_end)] # zip and unpack into a list the dates to obtain pairs of start-end for each weekend\n",
    "\n",
    "        return weekends_zipped\n",
    "    \n",
    "\n",
    "        \n",
    "    def set_flight_ids(self, count):\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        Helper method used to get unique id identifiers for the scraped flights.\n",
    "\n",
    "        Parameters: \n",
    "            count : int\n",
    "            Number of flights to generate ids for\n",
    "\n",
    "        Returns:\n",
    "            ids : List[UUID]\n",
    "            List containing pairs of unique identifiers \n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        ids = []\n",
    "\n",
    "        for i in range(count):\n",
    "            ids += [uuid.uuid4()]*2 # For each trip 2 of the same id are generated, one for the outbound and one for the inbound flight\n",
    "        return ids\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def scrape_data(self):\n",
    "\n",
    "        \"\"\" \n",
    "        \n",
    "        Main class method used for scraping data and collecting it in the list attributes. \n",
    "        Only the first 3 result (6 flights) of each page are scraped.\n",
    "\n",
    "        Parameters: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "\n",
    "\n",
    "        \"\"\" \n",
    "\n",
    "    \n",
    "        weekends_list = self.get_weekends() # First the helper function get_weekends is used to obtain the weekends betweeen given dates\n",
    "\n",
    "        for start, end in weekends_list: # For each weekend attempt to scrape the corresponding website\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            for i in range(5): # To avoid problems with website loading incorectly, attempt to scrape each website up to 5 times\n",
    "\n",
    "\n",
    "                try:\n",
    "                    print(f'Scraping data for dates: {start.strftime(\"%Y-%m-%d\")} - {end.strftime(\"%Y-%m-%d\")} and locations {self.origin} -> {self.destination}.\\n Scraping progress: {int(weekends_list.index((start,end)))/len(weekends_list)*100}%')\n",
    "\n",
    "                    self.get_page(start_date=start.strftime('%Y-%m-%d'), end_date=end.strftime('%Y-%m-%d')) # Load the page using helper method get_page\n",
    "                    \n",
    "                    \n",
    "                    time.sleep(1) # After each operation a wait time of 1s is added to ensure all data is scraped properly\n",
    "\n",
    "                    flight_times =  [timestamp.text for timestamp in self._driver.find_elements_by_xpath('//time[@datetime]')][0:18] # For 6 flights we need total of 18 datetime objects\n",
    "\n",
    "                    flight_start_times = flight_times[::3]\n",
    "                    flight_end_times = flight_times[2::3]\n",
    "                    flight_durations = flight_times[1::3]\n",
    "                    \n",
    "                    \n",
    "                    time.sleep(1)\n",
    "\n",
    "                    flight_prices = [price.text for price in \n",
    "                                     self._driver.find_elements_by_xpath('//span[contains(@class, \"length-6\") or contains(@class, \"length-7\")]') \n",
    "                                     for _ in (0,1)][0:6] # Since each price appears only once in each search result it is needed to duplicate them for data integrity purposes\n",
    "                    \n",
    "\n",
    "                    time.sleep(1)\n",
    "                    \n",
    "                    # For 6 flights we need total of 12 airports\n",
    "                    flight_destinations = [dest.text for dest in self._driver.find_elements_by_xpath('//div[contains(@data-test, \"stationName\")]')][0:12] \n",
    "\n",
    "                    flight_origin_airports = flight_destinations[::2]\n",
    "                    flight_destination_airports = flight_destinations[1::2]\n",
    "\n",
    "\n",
    "                    time.sleep(1)\n",
    "\n",
    "                    flight_grounds = self.find_grounds()[0:6] # Find the grounds for scraped flights, 6 in total\n",
    "\n",
    "                    time.sleep(1)\n",
    "\n",
    "\n",
    "                    # If the scraping was successful, the following need to be true: No information is missing, and every piece of information was scraped for exactly 6 flights\n",
    "                    if all([flight_start_times,flight_end_times,flight_durations,flight_prices,flight_origin_airports,flight_destination_airports]):\n",
    "                        if all(6==len(x) for x in [flight_start_times,flight_end_times,flight_durations,flight_prices,flight_origin_airports,flight_destination_airports]):\n",
    "\n",
    "\n",
    "                            # In case the scraping was successful, the scraped data can be added to the list attributes of the scraper\n",
    "                            print(f\"Scraping for date {start} - {end} successful!\")\n",
    "\n",
    "\n",
    "                            self._start_location += [self.origin] * 6\n",
    "                            self._end_location += [self.destination] * 6\n",
    "                            \n",
    "                            self._flight_start_times += flight_start_times\n",
    "                            self._flight_end_times += flight_end_times\n",
    "                            self._flight_durations += flight_durations\n",
    "\n",
    "                            self._flight_prices += flight_prices\n",
    "\n",
    "                            self._flight_origin_airports += flight_origin_airports\n",
    "                            self._flight_destination_airports += flight_destination_airports\n",
    "\n",
    "                            self._flight_grounds += flight_grounds\n",
    "\n",
    "\n",
    "                            # The folowing data is not scraped from the webiste, it is added manually for convenience purposes\n",
    "                            count = len(flight_times[::3])//2 # Number of scraped trips\n",
    "\n",
    "                            self._flight_ids += self.set_flight_ids(count) # Generate unique ids for each flight (pairwise matching)\n",
    "\n",
    "                            self._flight_start_dates += [start.strftime('%Y-%m-%d')]  * 2 * count # Start and end dates are set based on the initial conditions\n",
    "                            self._flight_end_dates += [end.strftime('%Y-%m-%d')]  * 2 * count\n",
    "\n",
    "                            self._is_return += [False,True] * count # The return flights are marked to allow for later distinction\n",
    "\n",
    "                            \n",
    "                            break # In case the scraping was successful break out of the inner loop and continue scraping for next dates \n",
    "                        else:\n",
    "                            print(f\"There was an error in attempt {i+1}/5, retrying...\")\n",
    "                    else:\n",
    "                        print(f\"There was an error in attempt {i+1}/5, retrying...\")\n",
    "\n",
    "\n",
    "                except:\n",
    "                    print(f\"Error occured for this date {start} - {end}\")\n",
    "                    continue # In case all 5 of the scraping attempts failed, continue scraping for the next dates and pring the message below\n",
    "\n",
    "\n",
    "                if not all([flight_start_times,flight_end_times,flight_durations,flight_prices,flight_origin_airports,flight_destination_airports]):\n",
    "                        if not all(6==len(x) for x in [flight_start_times,flight_end_times,flight_durations,flight_prices,flight_origin_airports,flight_destination_airports]):\n",
    "                            print(f\"Scraping for date {start} - {end} unsuccesfull :()\")\n",
    "\n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    def get_dataframe(self):\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        Helper method used to compose scraped data into a pandas DataFrame.\n",
    "\n",
    "        Parameters: \n",
    "            None\n",
    "\n",
    "        Returns:\n",
    "            flight_data : pd.DataFrame()\n",
    "            pandas DataFrame object containing raw data scraped for each of the list attributes\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        flight_data = pd.DataFrame({\n",
    "            'Flight_id': self._flight_ids,\n",
    "            'Start_location': self._start_location,\n",
    "            'End_location': self._end_location,\n",
    "            'Start_date': self._flight_start_dates,\n",
    "            'End_date': self._flight_end_dates,\n",
    "            \"Start_time\": self._flight_start_times,\n",
    "            \"End_time\": self._flight_end_times,\n",
    "            \"Duration\": self._flight_durations,\n",
    "            \"Price\": self._flight_prices,\n",
    "            \"Origin_airport\": self._flight_origin_airports,\n",
    "            \"Destinantion_airport\": self._flight_destination_airports,\n",
    "            'Number_of_grounds': self._flight_grounds,\n",
    "            \"Is_return\": self._is_return\n",
    "        })\n",
    "\n",
    "        return flight_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_list = ['katowice-polska','wroclaw-polska','krakow-polska','warszawa-polska']\n",
    "destination_list = ['londyn-wielka-brytania', 'barcelona-hiszpania', 'nowy-jork-nowy-jork-stany-zjednoczone']\n",
    "\n",
    "\n",
    "def scrape_data(origin_list, destination_list, date_from, date_to):\n",
    "\n",
    "    trip_duration_days = 3\n",
    "\n",
    "    path_to_driver = r'C://Users//mrceb//Desktop//chromedriver-win64//chromedriver.exe'\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    \n",
    "    for origin in origin_list:\n",
    "        for destination in destination_list:\n",
    "            \n",
    "            Scraper = FlightScraper(date_from=date_from, date_to=date_to, origin=origin, destination=destination, trip_duration_days=trip_duration_days, path_to_driver=path_to_driver)\n",
    "            Scraper.scrape_data()\n",
    "            print([len(x) for x in Scraper.__dict__.values() if isinstance(x,list)])\n",
    "            df_list.append(Scraper.get_dataframe())\n",
    "\n",
    "\n",
    "    return (pd.concat(df_list))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrceb\\AppData\\Local\\Temp\\ipykernel_20428\\1845722283.py:85: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  self._driver = webdriver.Chrome(executable_path=path_to_driver)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for dates: 2024-02-02 - 2024-02-04 and locations katowice-polska -> londyn-wielka-brytania.\n",
      " Scraping progress: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrceb\\AppData\\Local\\Temp\\ipykernel_20428\\1845722283.py:127: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  self._driver.find_element_by_xpath('//*[@id=\"cookies_accept\"]/div').click()\n",
      "C:\\Users\\mrceb\\AppData\\Local\\Temp\\ipykernel_20428\\1845722283.py:272: DeprecationWarning: find_elements_by_xpath is deprecated. Please use find_elements(by=By.XPATH, value=xpath) instead\n",
      "  flight_times =  [timestamp.text for timestamp in self._driver.find_elements_by_xpath('//time[@datetime]')][0:18] # For 6 flights we need total of 18 datetime objects\n",
      "C:\\Users\\mrceb\\AppData\\Local\\Temp\\ipykernel_20428\\1845722283.py:282: DeprecationWarning: find_elements_by_xpath is deprecated. Please use find_elements(by=By.XPATH, value=xpath) instead\n",
      "  self._driver.find_elements_by_xpath('//span[contains(@class, \"length-6\") or contains(@class, \"length-7\")]')\n",
      "C:\\Users\\mrceb\\AppData\\Local\\Temp\\ipykernel_20428\\1845722283.py:289: DeprecationWarning: find_elements_by_xpath is deprecated. Please use find_elements(by=By.XPATH, value=xpath) instead\n",
      "  flight_destinations = [dest.text for dest in self._driver.find_elements_by_xpath('//div[contains(@data-test, \"stationName\")]')][0:12]\n",
      "C:\\Users\\mrceb\\AppData\\Local\\Temp\\ipykernel_20428\\1845722283.py:153: DeprecationWarning: find_elements_by_xpath is deprecated. Please use find_elements(by=By.XPATH, value=xpath) instead\n",
      "  parent_divs = self._driver.find_elements_by_xpath(parent_div_xpath)\n",
      "C:\\Users\\mrceb\\AppData\\Local\\Temp\\ipykernel_20428\\1845722283.py:155: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  child_divs = [parent_div.find_element_by_xpath(child_div_xpath) for parent_div in parent_divs] # For each flight result card find a grounds counter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping for date 2024-02-02 00:00:00 - 2024-02-04 00:00:00 successful!\n",
      "Scraping data for dates: 2024-02-09 - 2024-02-11 and locations katowice-polska -> londyn-wielka-brytania.\n",
      " Scraping progress: 3.3333333333333335%\n",
      "Error occured for this date 2024-02-09 00:00:00 - 2024-02-11 00:00:00\n",
      "Scraping data for dates: 2024-02-09 - 2024-02-11 and locations katowice-polska -> londyn-wielka-brytania.\n",
      " Scraping progress: 3.3333333333333335%\n",
      "Scraping for date 2024-02-09 00:00:00 - 2024-02-11 00:00:00 successful!\n",
      "Scraping data for dates: 2024-02-16 - 2024-02-18 and locations katowice-polska -> londyn-wielka-brytania.\n",
      " Scraping progress: 6.666666666666667%\n",
      "Scraping for date 2024-02-16 00:00:00 - 2024-02-18 00:00:00 successful!\n",
      "Scraping data for dates: 2024-02-23 - 2024-02-25 and locations katowice-polska -> londyn-wielka-brytania.\n",
      " Scraping progress: 10.0%\n",
      "Scraping for date 2024-02-23 00:00:00 - 2024-02-25 00:00:00 successful!\n",
      "Scraping data for dates: 2024-03-01 - 2024-03-03 and locations katowice-polska -> londyn-wielka-brytania.\n",
      " Scraping progress: 13.333333333333334%\n",
      "Scraping for date 2024-03-01 00:00:00 - 2024-03-03 00:00:00 successful!\n",
      "Scraping data for dates: 2024-03-08 - 2024-03-10 and locations katowice-polska -> londyn-wielka-brytania.\n",
      " Scraping progress: 16.666666666666664%\n",
      "Scraping for date 2024-03-08 00:00:00 - 2024-03-10 00:00:00 successful!\n",
      "Scraping data for dates: 2024-03-15 - 2024-03-17 and locations katowice-polska -> londyn-wielka-brytania.\n",
      " Scraping progress: 20.0%\n"
     ]
    }
   ],
   "source": [
    "df = scrape_data(origin_list=origin_list, destination_list=destination_list, date_from='2024-01-28', date_to='2024-08-31')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Flight_id               0\n",
       "Start_location          0\n",
       "End_location            0\n",
       "Start_date              0\n",
       "End_date                0\n",
       "Start_time              0\n",
       "End_time                0\n",
       "Duration                0\n",
       "Price                   0\n",
       "Origin_airport          0\n",
       "Destinantion_airport    0\n",
       "Number_of_grounds       0\n",
       "Is_return               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
